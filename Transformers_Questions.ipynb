{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 How does a Transformer Work?\n",
    "\n",
    "A transformer is a type of neural network architecture that is designed to handle sequential data. It is more efficient than RNNs and LSTMs. the transformer architecture was introduced in the papper Attention is All you need. It is the base architecture for state of the art models such as BERT, GPT, T5.\n",
    "\n",
    "Architecture Overview: It is a Encoder-Decoder Structure: The transformer consists of an encoder and a decoder Howerver in some models like Bert only the encoder is used. in others uch as GPT only the decoder is used.\n",
    "\n",
    "Encoder: Process the input data and generates a representation of it. \n",
    "Decoder: Takes encoder's ouput and generates the desired ouput.\n",
    "\n",
    "## Self attention Mechanism\n",
    "\n",
    "The core innovation in transformers is the self-attention mechanism. Which allows the model to weigh the importance of different words in a sentence relative to each other. This mechanism enables the model to capture long range dependdencies and relationships between wordds, regardless of ther position in the input sequence. \n",
    "\n",
    "Attention Scores The self attention scores for each word in the input determining howmmuch focus to give to other words in sequence when enconding a particular word.\n",
    "\n",
    "Multihead attention, instead of computing a single set of attentions scores, the transformers use multiple attention head  eah lieanrning differnt aspects of the input sequence . These are then combined  to produce richer representations. \n",
    "\n",
    "## Positional Encoding\n",
    "\n",
    "\n",
    "## Feed-Forward Networks\n",
    "\n",
    "## Layer Normalization and Residual Connections\n",
    "\n",
    "\n",
    "## Output Generation\n",
    "\n",
    "\n",
    "# Training\n",
    "\n",
    "# Advantages of Transformers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what is a word embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whats the multihead attention in transformers do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What are common ways to deal with the vanishing gradient problem in large neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "How do agents in LangChain work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What are common metrics for Classification and Regression Problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Area Under the ROC curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "How Does RAG Work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "How would you write a query for table databases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
